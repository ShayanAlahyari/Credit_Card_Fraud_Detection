{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, precision_score, recall_score, \n",
    "    f1_score, confusion_matrix, classification_report, roc_curve\n",
    ")\n",
    "import smote_variants as sv\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import InputLayer, Dense, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Load and preprocess data\n",
    "data = pd.read_csv('creditcard.csv')\n",
    "scaler_amount = RobustScaler()\n",
    "scaler_time = StandardScaler()\n",
    "data['Amount'] = scaler_amount.fit_transform(data['Amount'].values.reshape(-1, 1))\n",
    "data['Time'] = scaler_time.fit_transform(data['Time'].values.reshape(-1, 1))\n",
    "data = data.sample(frac=1, random_state=1)\n",
    "\n",
    "X = data.drop('Class', axis=1)\n",
    "y = data['Class']\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Visualizations\n",
    "plt.figure(figsize=(20, 8))\n",
    "sns.boxplot(data=X[['Time', 'Amount']])\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(data.corr(), cmap='coolwarm')\n",
    "plt.show()\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(X)\n",
    "plt.scatter(pca_result[:, 0], pca_result[:, 1], c=y, cmap='coolwarm', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "clusters = kmeans.fit_predict(X)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(pca_result[:, 0], pca_result[:, 1], c=clusters, cmap='viridis')\n",
    "plt.show()\n",
    "\n",
    "stats.probplot(data['Amount'], dist=\"norm\", plot=plt)\n",
    "plt.show()\n",
    "\n",
    "# Helper functions\n",
    "def evaluate_model(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    y_prob = model.predict_proba(X)[:, 1] if hasattr(model, 'predict_proba') else model.predict(X)\n",
    "    return {\n",
    "        'ROC AUC': roc_auc_score(y, y_prob),\n",
    "        'Precision': precision_score(y, y_pred),\n",
    "        'Recall': recall_score(y, y_pred),\n",
    "        'F1 Score': f1_score(y, y_pred),\n",
    "        'Confusion Matrix': confusion_matrix(y, y_pred)\n",
    "    }\n",
    "\n",
    "def apply_smote(X, y, method):\n",
    "    oversampler = getattr(sv, method)()\n",
    "    return oversampler.sample(X, y)\n",
    "\n",
    "# Classifiers and Augmentation Methods\n",
    "classifiers = [\n",
    "    LogisticRegression(max_iter=1000),\n",
    "    RandomForestClassifier(),\n",
    "    GradientBoostingClassifier(),\n",
    "    SVC(probability=True),\n",
    "    XGBClassifier()\n",
    "]\n",
    "\n",
    "augmentation_methods = ['SMOTE', 'SMOTE_TomekLinks', 'SMOTE_ENN']\n",
    "results_before_aug = {}\n",
    "results_after_aug = {}\n",
    "\n",
    "# Train and Evaluate Models Before Augmentation\n",
    "for clf in classifiers:\n",
    "    clf_name = clf.__class__.__name__\n",
    "    clf.fit(X_train, y_train)\n",
    "    results_before_aug[clf_name] = evaluate_model(clf, X_val, y_val)\n",
    "\n",
    "# Train and Evaluate Models After Augmentation\n",
    "for method in augmentation_methods:\n",
    "    X_aug, y_aug = apply_smote(X_train, y_train, method)\n",
    "    aug_results = {}\n",
    "    for clf in classifiers:\n",
    "        clf_name = clf.__class__.__name__\n",
    "        clf.fit(X_aug, y_aug)\n",
    "        aug_results[clf_name] = evaluate_model(clf, X_val, y_val)\n",
    "    results_after_aug[method] = aug_results\n",
    "\n",
    "# Print Results\n",
    "print(\"Comparison of models before augmentation:\")\n",
    "for clf_name, metrics in results_before_aug.items():\n",
    "    print(f\"\\nClassifier: {clf_name}\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "for method, models in results_after_aug.items():\n",
    "    print(f\"\\nResults for augmentation method: {method}\")\n",
    "    for clf_name, metrics in models.items():\n",
    "        print(f\"\\nClassifier: {clf_name}\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"{metric}: {value}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Neural Network Training\n",
    "def create_neural_network(input_shape):\n",
    "    model = Sequential([\n",
    "        InputLayer(input_shape=(input_shape,)),\n",
    "        Dense(16, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "neural_network = create_neural_network(X_train.shape[1])\n",
    "checkpoint = ModelCheckpoint('best_nn_model.h5', save_best_only=True)\n",
    "neural_network.fit(\n",
    "    X_train, y_train, validation_data=(X_val, y_val), \n",
    "    epochs=10, batch_size=64, callbacks=[checkpoint]\n",
    ")\n",
    "nn_metrics = evaluate_model(neural_network, X_val, y_val)\n",
    "print(\"\\nNeural Network Results:\")\n",
    "for metric, value in nn_metrics.items():\n",
    "    print(f\"{metric}: {value}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# XGBoost with Grid Search\n",
    "xgb_model = XGBClassifier()\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2]\n",
    "}\n",
    "grid_search = GridSearchCV(xgb_model, param_grid, cv=3, scoring='roc_auc', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_model = grid_search.best_estimator_\n",
    "xgb_metrics = evaluate_model(best_model, X_val, y_val)\n",
    "print(\"\\nXGBoost with GridSearch Results:\")\n",
    "for metric, value in xgb_metrics.items():\n",
    "    print(f\"{metric}: {value}\")\n",
    "print(\"-\" * 50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
